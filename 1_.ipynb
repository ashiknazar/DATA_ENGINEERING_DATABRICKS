{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/datal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learn how to use databrick env\n",
    "  - create cluster\n",
    "  - switch on dbfs \n",
    "  - upload required csv,\n",
    "  create delta table\n",
    "- understand lld(low level design) of writing good codebase in data engineering\n",
    "  - understand factory pattern\n",
    "  - create one abstract reader which will use factory pattern\n",
    "- design code base\n",
    "  - required extractor,transformer&loader class\n",
    "  - required utils class\n",
    "- spark concept\n",
    "  - sparkSession\n",
    "  - row vs column file formats\n",
    "  - broadcast join\n",
    "  - narrow vs wide transformations\n",
    "  - job status & task\n",
    "- apache spark concepts\n",
    "  - repartition vs coalesce\n",
    "  - partitionBy vs Bucketing\n",
    "  - spark ui\n",
    "  - datalake vs datalakehouse\n",
    "  - using optimizing technique like predicate pushdown,predicate pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- different business logic\n",
    "   - customer who have bought airpods after buying iphone\n",
    "   - customer who have bought both airpods and iphone\n",
    "   - list all the products bought by customers after their initial purchase\n",
    "   - determine the average time delay buying an iphone and buying airpods for each customers\n",
    "   - identify the top 3 selling products in each category by total revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SETTINGS -> advanced -> search DBFS -> TURN ON (Enable or disable DBFS browser)\n",
    "    - You can browse and manage files stored in DBFS via the Databricks UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute (create first cluster)\n",
    "  - create compute \n",
    "     - compute name (`cluster1`)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- catalog -> DBFS -> FileStore -> tables  -> upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- workspace -> users -> user@gmail.com -> user@gmail.com -> dropdown -> create -> Folder (`appleAnalysis` )->create folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- workspace -> users -> user@gmail.com -> user@gmail.com -> (`appleAnalysis`) -> create -> notebook(`apple_analysis`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check wether cluster is created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- catalog - > database tables -> default -> create table -> DBFS -> FileStore -> tables -> select(customer_update.csv)->create table with ui  - (created delta table)\n",
    "- select a cluster to preview the table\n",
    "  - cluster1\n",
    "- preview table\n",
    "  - set first row as header\n",
    "- create table\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apple_analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "\n",
    "\n",
    "%run \"./reader_factory\"   # to import from ipynb\n",
    "\n",
    "%run \"./transform\"\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def runner(self):\n",
    "        transactionInputDF=get_data_source(data_type=\"csv\",\n",
    "        file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\").get_data_frame()\n",
    "        transactionInputDF.show()\n",
    "        inputDFs={\"transactionInputDF\":transctionInputDF}\n",
    "        #customers who bought airpods after iphone\n",
    "        firstTransform=FirstTransformer().transform(inputDFs)\n",
    "\n",
    "workFlow=Workflow().runner()\n",
    "\n",
    "\n",
    "\n",
    "#LEAD - > PARTITION BY CUSTOMER ID ,order by transaction_data asc\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"thebigdata.me\").getOrCreate()\n",
    "input_df=(\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",True)\n",
    "    .load(\"dbfs:/FileSore/tables/Transaction_Updated.csv\")\n",
    ")\n",
    "input_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create another notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reader_factory.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class DataSource:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    def get_data_frame(self):\n",
    "        raise ValueError(\"not implemented\")\n",
    "class CSVDataSource(DataSource):\n",
    "    def get_data_frame(self):\n",
    "        return(spark.read.format(\"csv\").option(\"header\",True).load(self.path))\n",
    "class ParquetDataSource(DataSource):\n",
    "    def get_data_frame(self):\n",
    "        return(spark.read.format(\"parquet\").load(self.path))\n",
    "class DeltaDataSource(DataSource):\n",
    "    def get_data_frame(self):\n",
    "\n",
    "        table_name=self.path\n",
    "        return(spark.read.format(\"delta\").load(table_name))\n",
    "\n",
    "\n",
    "def get_data_source(data_type,file_path):\n",
    "    if data_type==\"csv\":\n",
    "        return CSVDataSource(file_path)\n",
    "    elif data_type==\"parquet\":\n",
    "        return ParquetDataSource(file_path)\n",
    "    elif data_type==\"delta\":\n",
    "        return DeltaDataSource(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a job in spark represents a computation triggered by an action such as count(),collect(),save(),etc\n",
    "- when you perform an action on a dataframe or rdd ,spark submits a job\n",
    "- a stage consist of sequence of transformation that can be formed without shuffling entire datset across the partitions\n",
    "- stages are divided by transformations that requires a shuffle such as groupBy(),reduceByKey() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each stage has its own set of tasks that execute the same code but on different partitions of dataset ,and spark tries to minimize shuffling between stages to optimize performance\n",
    "- task is the smallest unit of work in spark.it represents the computation performed on a single partition of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when spark executes a stage,it devides the data into tasks ,each of which processes a slice of data in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extractor.ipynb`\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def extract(self,):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "`transform.ipynb`\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead,col\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self,inputDF):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self,inputDF):\n",
    "        transactionInputDF=inputDFs.get(\"transactioninputDF\")\n",
    "        print(\"transactioninputDF in transform\")\n",
    "        transactionInputDF.show()\n",
    "\n",
    "        windowSpec=Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "        transformedDF=transactionInputDF.withColumn(\n",
    "            \"next_product_name\",lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "        print(\"airpod after iphone\")\n",
    "        transformedDF.orderBy(\"customer_id\",\"transaction_date\",\"product_name\").show() \n",
    "        filteredDF= transformedDF.filter((col(\"product_name\")==\"iPhone\") & (col(\"next_product_name\")==\"AirPods\"))\n",
    "        filteredDF.orderBy(\"customer_id\",\"transaction_date\",\"product_name\").show()\n",
    "        joinDF=\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loader.ipynb`\n",
    "```python\n",
    "class Loader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sink(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LEAD() / LAG()\n",
    "   - IN MYSQL USED TO GET preceding and succeeding value of any row within its partition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "file_location=\"/FileStore/tables/Customer_Updated.csv\"\n",
    "file_type=\"csv\"\n",
    "infer_schema=\"true\"\n",
    "first_row_is_header=\"true\"\n",
    "delimiter=\",\"\n",
    "df=spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\",infer_schema) \\\n",
    "  .option(\"header\",first_row_is_header) \\\n",
    "  .option(\"sep\",delimiter) \\\n",
    "  .load(file_location)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "temp_table_name=\"customer_delta_table\"\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sql\n",
    "select * from `Customer_delta_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "permanent_table_name=\"Customer_delta_table_persist\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
